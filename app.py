import streamlit as st
import pandas as pd
import google.generativeai as genai
from streamlit_gsheets import GSheetsConnection
import matplotlib.pyplot as plt
import seaborn as sns

# --- CONFIGURACI√ìN VISUAL ---
st.set_page_config(page_title="Analista ONErpm AI", page_icon="üéπ", layout="centered")
st.title("üéπ Chat con Datos ONErpm")
st.markdown("---")

# --- 1. CONEXI√ìN API ---
try:
    genai.configure(api_key=st.secrets["GOOGLE_API_KEY"])
except Exception:
    st.error("‚ö†Ô∏è Error: No se detect√≥ la API Key en los Secrets.")
    st.stop()

# --- 2. CARGA DE DATOS ---
url_sheet = "https://docs.google.com/spreadsheets/d/10y2YowTEgQYdWxs6c8D0fgJDDwGIT8_wyH0rQbERgG0/edit?gid=1919114384#gid=1919114384"

@st.cache_data(ttl=600)
def load_data():
    conn = st.connection("gsheets", type=GSheetsConnection)
    try:
        df = conn.read(spreadsheet=url_sheet, worksheet="DSP COPY")
        # Limpieza autom√°tica de fechas
        if 'Release Date' in df.columns:
            df['Release Date'] = pd.to_datetime(df['Release Date'], errors='coerce')
        return df
    except Exception as e:
        st.error(f"Error conectando a Sheets: {e}")
        return None

with st.spinner('Conectando con la nube...'):
    df = load_data()

# --- 3. L√ìGICA DEL CHAT VISUAL ---
if df is not None:
    # Mensaje de bienvenida
    if "messages" not in st.session_state:
        st.session_state.messages = []
        st.session_state.messages.append({"role": "assistant", "content": "Hola üëã. Preg√∫ntame lo que quieras. Puedo generar **tablas** y **gr√°ficas**."})

    # Mostrar historial
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            # Si el contenido es texto, lo mostramos
            if isinstance(message["content"], str):
                st.markdown(message["content"])
            # (Nota: Las gr√°ficas pasadas no se guardan en historial simple para ahorrar memoria, 
            # pero las nuevas se generar√°n al momento)

    # Input del usuario
    if prompt := st.chat_input("Ej: Haz una gr√°fica de torta comparando Spotify 2025 vs 2026"):
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)

        with st.chat_message("assistant"):
            caja_loading = st.empty()
            caja_loading.markdown("üé® *Dise√±ando respuesta visual...*")

            try:
                # --- CEREBRO: INSTRUCCIONES PARA STREAMLIT ---
                info_columnas = df.dtypes.to_markdown()
                head_data = df.head(3).to_markdown(index=False)

                prompt_maestro = f"""
                Act√∫a como un Cient√≠fico de Datos experto usando Streamlit.
                Tienes un DataFrame `df`.
                Metadata: {info_columnas}
                Muestra: {head_data}
                
                Usuario pide: "{prompt}"
                
                TU TAREA:
                Genera c√≥digo Python que se ejecutar√° dentro de una app Streamlit.
                
                REGLAS OBLIGATORIAS:
                1. PARA TEXTO: Usa `st.write("Texto")` o `st.success("Dato")`. NO uses print().
                2. PARA TABLAS: Usa `st.dataframe(df_resultado)`.
                3. PARA GR√ÅFICAS:
                   - Usa `fig, ax = plt.subplots()`
                   - Usa seaborn (`sns`) o matplotlib.
                   - AL FINAL DE LA GR√ÅFICA: usa `st.pyplot(fig)`.
                   - NO uses plt.show().
                4. Si calculas un porcentaje, mu√©stralo claro con `st.metric()`.
                5. Importa lo necesario dentro del c√≥digo si hace falta.
                
                Dame SOLO el c√≥digo, sin ```python al inicio.
                """

                # Intentamos con Flash, si falla vamos a Pro
                try:
                    model = genai.GenerativeModel('gemini-1.5-flash')
                    response = model.generate_content(prompt_maestro)
                except:
                    model = genai.GenerativeModel('gemini-pro')
                    response = model.generate_content(prompt_maestro)

                codigo = response.text.replace("```python", "").replace("```", "").replace("plt.show()", "#plt.show() anulado").strip()
                
                # Limpiamos el mensaje de carga
                caja_loading.empty()
                
                # --- EJECUCI√ìN VISUAL ---
                # Pasamos las librer√≠as necesarias al entorno de ejecuci√≥n
                local_vars = {
                    "df": df, 
                    "pd": pd, 
                    "st": st, 
                    "plt": plt, 
                    "sns": sns
                }
                exec(codigo, {}, local_vars)
                
                # Guardamos solo el texto del prompt en historial para referencia
                st.session_state.messages.append({"role": "assistant", "content": "‚úÖ An√°lisis visual generado arriba."})

            except Exception as e:
                caja_loading.error(f"Hubo un error t√©cnico: {str(e)}")
                with st.expander("Ver detalle del error"):
                    st.write(e)
